{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data used in this simulation, download from https://www.openicpsr.org/openicpsr/project/116357/version/V1/view. \n",
    "\n",
    "After downloading, combine the $7$ data files into one csv and add to `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "from process_meager import get_meager_cluster_df, get_meager_cluster_dict  \n",
    "\n",
    "os.chdir('../..')\n",
    "from notebooks.meager_nhis.cluster_env import ClusterEnv\n",
    "from aexgym.model import fixedPersonalizedModel \n",
    "from scripts.setup_script import make_uniform_prior\n",
    "from aexgym.env import BaseContextualEnv\n",
    "from aexgym.agent import LinearUniform, LinearTS, LinearUCB, LinearEI\n",
    "from aexgym.objectives import contextual_simple_regret\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwang/repos/aexgym/notebooks/meager_nhis/process_meager.py:24: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATAPATH)\n"
     ]
    }
   ],
   "source": [
    "# make dataframe and dictionary\n",
    "df = get_meager_cluster_df(DATAPATH = 'data/Meager_data.csv', cluster_type = 'district', filter_both_treat=True) \n",
    "meager_dict = get_meager_cluster_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward types: Index(['profit', 'expenditures', 'revenues', 'consumption'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"reward types:\", meager_dict[0]['response'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[1.]]),\n",
       "  tensor([[1.0000, 1.0000, 0.2800,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [1.0000, 1.0000, 0.5800,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          [1.0000, 1.0000, 0.2300,  ..., 0.0000, 1.0000, 0.0000],\n",
       "          ...,\n",
       "          [1.0000, 0.1800, 0.5200,  ..., 0.0000, 0.0000, 1.0000],\n",
       "          [1.0000, 0.2200, 0.5400,  ..., 0.0000, 0.0000, 1.0000],\n",
       "          [1.0000, 0.3800, 0.4700,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "         dtype=torch.float64),\n",
       "  tensor([[1.]])),\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup parameters for env \n",
    "n_steps = 32\n",
    "budget = False \n",
    "no_duplicates = False \n",
    "reward_type = ['profit']\n",
    "cluster_batch_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "#initialize env \n",
    "env = ClusterEnv(\n",
    "    cluster_dict = meager_dict,\n",
    "    batch_size = batch_size, \n",
    "    cluster_batch_size = cluster_batch_size,\n",
    "    n_steps = n_steps, \n",
    "    reward_type=reward_type, \n",
    "    no_duplicates=no_duplicates, \n",
    "    budget=budget\n",
    ")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup parameters for agent and model \n",
    "n_objs = 1\n",
    "scaling = 0.01\n",
    "s2 = scaling*torch.ones(100, n_objs)\n",
    "n_arms = env.temp_feature_list.shape[0]\n",
    "beta, sigma = make_uniform_prior(2*env.temp_feature_list.shape[1], scaling, n_objs=n_objs)\n",
    "\n",
    "\n",
    "model = fixedPersonalizedModel(\n",
    "    beta_0 = beta, \n",
    "    sigma_0 = sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2, \n",
    "    n_objs=n_objs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agents \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples=1, constraint=False)\n",
    "agent = LinearUCB(model, \"Linear UCB\", alpha=0.95) \n",
    "agent = LinearEI(model, \"Linear EI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regret:  173.35816574402742\n",
      "Percent Arms Correct:  0.0\n",
      "Regret:  161.45999528650145\n",
      "Percent Arms Correct:  0.0\n",
      "Regret:  183.89542680462355\n",
      "Percent Arms Correct:  0.0\n",
      "Regret:  186.31789945757376\n",
      "Percent Arms Correct:  0.03225806451612903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mn_arms \u001b[38;5;241m=\u001b[39m action_contexts\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     40\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     41\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[1;32m     42\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m state_contexts, \n\u001b[1;32m     43\u001b[0m     action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     44\u001b[0m     objective \u001b[38;5;241m=\u001b[39m objective\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#print probabilities \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_probs \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "objective = contextual_simple_regret()\n",
    "num_experiments = 250\n",
    "print_probs = False\n",
    "\n",
    "\n",
    "    \n",
    "#set seed and experiment parameters \n",
    "torch.manual_seed(0)\n",
    "torch.set_printoptions(sci_mode=False) \n",
    "print_probs = False\n",
    "\n",
    "#set objective \n",
    "objective = contextual_simple_regret()\n",
    "\n",
    "#initialize lists\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "#run experiment simulation\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    beta_0, sigma_0 = beta.clone(), sigma.clone()\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "        \n",
    "        #update n_arms  \n",
    "        agent.model.n_arms = action_contexts.shape[0]\n",
    "\n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective\n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, torch.mean(probs, dim=0))\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        \n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta_0, \n",
    "            sigma = sigma_0, \n",
    "            rewards = sampled_rewards, \n",
    "            features = sampled_features, \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards()\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).view(1, agent.model.n_arms, 1).to(device), \n",
    "        true_rewards = true_eval_rewards.view(1, agent.model.n_arms, 1).to(device)\n",
    "    )\n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(\"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
