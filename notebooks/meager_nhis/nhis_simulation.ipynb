{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files can be downloaded at https://www.cdc.gov/nchs/nhis/data-questionnaires-documentation.htm\n",
    "\n",
    "After downloading data, follow the code instructions at https://github.com/sookyojeong/worst-ate \n",
    "to clean the data and put in in `data/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "from process_nhis import preprocess_nhis, get_nhis_cluster_dict\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('../..')\n",
    "\n",
    "from notebooks.meager_nhis.cluster_env import ClusterEnv\n",
    "from aexgym.model import fixedPersonalizedModel\n",
    "from scripts.setup_script import make_uniform_prior\n",
    "from aexgym.env import BaseContextualEnv\n",
    "from aexgym.agent import LinearUniform, LinearTS, LinearUCB, LinearEI\n",
    "from aexgym.objectives import contextual_simple_regret\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwang/repos/aexgym/notebooks/meager_nhis/process_nhis.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = 'data/nhis2017.csv'\n",
    "\n",
    "df = pd.read_csv(DATAPATH)\n",
    "df = preprocess_nhis(df)\n",
    "nhis_dict = get_nhis_cluster_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[1.]]),\n",
       "  tensor([[2.1300, 2.1900, 1.5300,  ..., 0.6300, 0.0300, 0.0300],\n",
       "          [1.8900, 2.6600, 1.5800,  ..., 0.6900, 0.0100, 0.0000],\n",
       "          [2.0700, 2.3600, 1.5500,  ..., 0.8400, 0.0100, 0.0100],\n",
       "          ...,\n",
       "          [1.9800, 2.4300, 1.6100,  ..., 0.7500, 0.0000, 0.0000],\n",
       "          [1.7700, 2.4700, 1.4900,  ..., 0.7100, 0.0200, 0.0200],\n",
       "          [1.9200, 2.4200, 1.4000,  ..., 0.7700, 0.0100, 0.0100]],\n",
       "         dtype=torch.float64),\n",
       "  tensor([[1.]])),\n",
       " 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 32\n",
    "budget = False \n",
    "no_duplicates = False \n",
    "reward_type = ['care_office_2wks']\n",
    "cluster_batch_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "env = ClusterEnv(\n",
    "    cluster_dict = nhis_dict, \n",
    "    batch_size = batch_size, \n",
    "    cluster_batch_size = cluster_batch_size, \n",
    "    n_steps = n_steps, \n",
    "    reward_type=reward_type, \n",
    "    no_duplicates=no_duplicates,\n",
    "    budget = budget)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objs = 1\n",
    "scaling = 0.01\n",
    "s2 = scaling*torch.ones(100, n_objs)\n",
    "n_arms = env.temp_feature_list.shape[0]\n",
    "beta, sigma = make_uniform_prior(2*env.temp_feature_list.shape[1], scaling, n_objs=n_objs)\n",
    "\n",
    "\n",
    "model = fixedPersonalizedModel(\n",
    "    beta_0 = beta, \n",
    "    sigma_0 = sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2, \n",
    "    n_objs=n_objs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agents \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples=1, constraint=False)\n",
    "#agent = LinearUCB(model, \"Linear UCB\", alpha=0.95) \n",
    "#agent = LinearEI(model, \"Linear EI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regret:  0.10999999999999999\n",
      "Percent Arms Correct:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m\n\u001b[1;32m     55\u001b[0m     all_contexts, sampled_rewards, sampled_features, cur_step  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     56\u001b[0m         state_contexts \u001b[38;5;241m=\u001b[39m state_contexts, \n\u001b[1;32m     57\u001b[0m         action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     58\u001b[0m         actions \u001b[38;5;241m=\u001b[39m actions\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m#update model state \u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     beta, sigma \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mupdate_posterior(\n\u001b[1;32m     63\u001b[0m         beta \u001b[38;5;241m=\u001b[39m beta_0, \n\u001b[1;32m     64\u001b[0m         sigma \u001b[38;5;241m=\u001b[39m sigma_0, \n\u001b[1;32m     65\u001b[0m         rewards \u001b[38;5;241m=\u001b[39m sampled_rewards, \n\u001b[1;32m     66\u001b[0m         features \u001b[38;5;241m=\u001b[39m sampled_features, \n\u001b[1;32m     67\u001b[0m         idx \u001b[38;5;241m=\u001b[39m cur_step\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#get evaluation contexts and true rewards \u001b[39;00m\n\u001b[1;32m     71\u001b[0m eval_contexts \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39msample_eval_contexts(access\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/model/base_model.py:99\u001b[0m, in \u001b[0;36mBaseLinearModel.update_posterior\u001b[0;34m(self, beta, sigma, rewards, features, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_posterior\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     94\u001b[0m                      beta:Tensor, \n\u001b[1;32m     95\u001b[0m                      sigma: Tensor, \n\u001b[1;32m     96\u001b[0m                      rewards: Tensor,\n\u001b[1;32m     97\u001b[0m                      features: Tensor,\n\u001b[1;32m     98\u001b[0m                      idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_linear_posterior(beta, sigma, rewards,  features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms2, idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_precision, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_objs, standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstandardize)\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/model/model_utils.py:39\u001b[0m, in \u001b[0;36mupdate_linear_posterior\u001b[0;34m(beta, sigma, rewards, features, s2, idx, use_precision, n_objs, standardize)\u001b[0m\n\u001b[1;32m     36\u001b[0m     get_sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m sigma, s2: sigma \u001b[38;5;241m+\u001b[39m XTX \u001b[38;5;241m/\u001b[39m (s2)\n\u001b[1;32m     37\u001b[0m     get_beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m beta, sigma, post_sigma, s2, XR: torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(post_sigma) \u001b[38;5;241m@\u001b[39m (XR \u001b[38;5;241m/\u001b[39m (s2) \u001b[38;5;241m+\u001b[39m sigma \u001b[38;5;241m@\u001b[39m beta)\n\u001b[0;32m---> 39\u001b[0m post_sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_sigma(sigma[:, :, i], s2[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     40\u001b[0m post_beta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_beta(beta[:, i], sigma[:, :, i], post_sigma[:, :, i], s2[i], XR[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m post_beta, post_sigma\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "objective = contextual_simple_regret()\n",
    "num_experiments = 250\n",
    "print_probs = False\n",
    "\n",
    "\n",
    "    \n",
    "#set seed and experiment parameters \n",
    "torch.manual_seed(0)\n",
    "torch.set_printoptions(sci_mode=False) \n",
    "print_probs = False\n",
    "\n",
    "#set objective \n",
    "objective = contextual_simple_regret()\n",
    "\n",
    "#initialize lists\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "#run experiment simulation\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    beta_0, sigma_0 = beta.clone(), sigma.clone()\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "        \n",
    "        #update n_arms  \n",
    "        agent.model.n_arms = action_contexts.shape[0]\n",
    "\n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective\n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, torch.mean(probs, dim=0))\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        \n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta_0, \n",
    "            sigma = sigma_0, \n",
    "            rewards = sampled_rewards, \n",
    "            features = sampled_features, \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards()\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).view(1, agent.model.n_arms, 1).to(device), \n",
    "        true_rewards = true_eval_rewards.view(1, agent.model.n_arms, 1).to(device)\n",
    "    )\n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(\"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
