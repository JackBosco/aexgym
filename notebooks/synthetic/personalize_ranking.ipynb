{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import PersSyntheticEnv, RankingSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel, PersonalizedRankingModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho, RankingUniform, RankingTS, RankingRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 8\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.1 * torch.ones((n_days, 1))\n",
    "\n",
    "n_items = 6\n",
    "total_items = 50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*100)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len, scaling, n_objs=n_objs)\n",
    "user_context_mu, user_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "item_context_mu, item_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "\n",
    "\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedRankingModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = RankingSyntheticEnv(\n",
    "    true_env = model,\n",
    "    n_steps = n_steps,\n",
    "    user_context_mu = user_context_mu, \n",
    "    user_context_var = user_context_var,\n",
    "    item_context_mu = item_context_mu,\n",
    "    item_context_var = item_context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size,\n",
    "    n_arms = n_arms,\n",
    "    n_items = n_items,\n",
    "    total_items = total_items\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 8]) torch.Size([100, 50, 8])\n",
      "6 torch.Size([10, 8])\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "contexts, cur_step = env.reset()\n",
    "state_contexts, action_contexts, eval_contexts = contexts \n",
    "user_contexts, item_contexts = state_contexts\n",
    "n_items, ranking_contexts = action_contexts\n",
    "print(user_contexts.shape, item_contexts.shape)\n",
    "print(n_items, ranking_contexts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent  \n",
    "agent = RankingUniform(model, \"Linear Uniform\")\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=False, n_samples = 1)\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "#agent = RankingRho(model, \"Linear Rho\", lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.01405759435147047\n",
      "Percent Arms Correct:  0.7\n",
      "1 Regret:  0.010629332158714533\n",
      "Percent Arms Correct:  0.725\n",
      "2 Regret:  0.011854222354789576\n",
      "Percent Arms Correct:  0.6833333333333332\n",
      "3 Regret:  0.010795207461342216\n",
      "Percent Arms Correct:  0.6875\n",
      "4 Regret:  0.016048735193908214\n",
      "Percent Arms Correct:  0.6519999999999999\n",
      "5 Regret:  0.017061014504482348\n",
      "Percent Arms Correct:  0.6466666666666666\n",
      "6 Regret:  0.016052419851933206\n",
      "Percent Arms Correct:  0.6571428571428571\n",
      "7 Regret:  0.014642253110650927\n",
      "Percent Arms Correct:  0.675\n",
      "8 Regret:  0.01399844755522079\n",
      "Percent Arms Correct:  0.6755555555555556\n",
      "9 Regret:  0.012962699262425303\n",
      "Percent Arms Correct:  0.687\n",
      "10 Regret:  0.013904563362964174\n",
      "Percent Arms Correct:  0.67\n",
      "11 Regret:  0.014679149840958416\n",
      "Percent Arms Correct:  0.6566666666666666\n",
      "12 Regret:  0.01467341030589663\n",
      "Percent Arms Correct:  0.643076923076923\n",
      "13 Regret:  0.01579371257685125\n",
      "Percent Arms Correct:  0.6214285714285713\n",
      "14 Regret:  0.015383812443663677\n",
      "Percent Arms Correct:  0.6273333333333333\n",
      "15 Regret:  0.017640557052800432\n",
      "Percent Arms Correct:  0.6100000000000001\n",
      "16 Regret:  0.017018970321206486\n",
      "Percent Arms Correct:  0.6188235294117648\n",
      "17 Regret:  0.01662277989089489\n",
      "Percent Arms Correct:  0.6238888888888889\n",
      "18 Regret:  0.016954705805370684\n",
      "Percent Arms Correct:  0.6205263157894737\n",
      "19 Regret:  0.01644624015316367\n",
      "Percent Arms Correct:  0.628\n",
      "20 Regret:  0.016155619351636796\n",
      "Percent Arms Correct:  0.63\n",
      "21 Regret:  0.01616285885260864\n",
      "Percent Arms Correct:  0.6245454545454545\n",
      "22 Regret:  0.016236324187206184\n",
      "Percent Arms Correct:  0.6234782608695653\n",
      "23 Regret:  0.016453926063453157\n",
      "Percent Arms Correct:  0.6179166666666666\n",
      "24 Regret:  0.016188771389424803\n",
      "Percent Arms Correct:  0.6247999999999999\n",
      "25 Regret:  0.016384680433055528\n",
      "Percent Arms Correct:  0.6215384615384614\n",
      "26 Regret:  0.016138280155482115\n",
      "Percent Arms Correct:  0.6229629629629628\n",
      "27 Regret:  0.01668649206736258\n",
      "Percent Arms Correct:  0.6107142857142857\n",
      "28 Regret:  0.01651095496169452\n",
      "Percent Arms Correct:  0.6155172413793103\n",
      "29 Regret:  0.01698500383645296\n",
      "Percent Arms Correct:  0.6133333333333333\n",
      "30 Regret:  0.0173521927527843\n",
      "Percent Arms Correct:  0.6096774193548387\n",
      "31 Regret:  0.016980788510409184\n",
      "Percent Arms Correct:  0.615625\n",
      "32 Regret:  0.017204849740885424\n",
      "Percent Arms Correct:  0.6124242424242424\n",
      "33 Regret:  0.01736704332699232\n",
      "Percent Arms Correct:  0.6088235294117647\n",
      "34 Regret:  0.017293416535747903\n",
      "Percent Arms Correct:  0.61\n",
      "35 Regret:  0.017071989972868726\n",
      "Percent Arms Correct:  0.6124999999999999\n",
      "36 Regret:  0.017200192776382774\n",
      "Percent Arms Correct:  0.6091891891891891\n",
      "37 Regret:  0.017341478911571596\n",
      "Percent Arms Correct:  0.6089473684210526\n",
      "38 Regret:  0.017198561129566185\n",
      "Percent Arms Correct:  0.612051282051282\n",
      "39 Regret:  0.017352991632651537\n",
      "Percent Arms Correct:  0.61\n",
      "40 Regret:  0.017480125366823702\n",
      "Percent Arms Correct:  0.6104878048780488\n",
      "41 Regret:  0.017956282444564358\n",
      "Percent Arms Correct:  0.6047619047619047\n",
      "42 Regret:  0.01762552136490338\n",
      "Percent Arms Correct:  0.6090697674418604\n",
      "43 Regret:  0.01758856882489371\n",
      "Percent Arms Correct:  0.6097727272727272\n",
      "44 Regret:  0.017933820493312345\n",
      "Percent Arms Correct:  0.6062222222222222\n",
      "45 Regret:  0.0185136604477125\n",
      "Percent Arms Correct:  0.5980434782608696\n",
      "46 Regret:  0.018852794240724217\n",
      "Percent Arms Correct:  0.5946808510638297\n",
      "47 Regret:  0.01914025212075406\n",
      "Percent Arms Correct:  0.591875\n",
      "48 Regret:  0.018839143991128217\n",
      "Percent Arms Correct:  0.5957142857142858\n",
      "49 Regret:  0.01891939306166023\n",
      "Percent Arms Correct:  0.5884\n",
      "50 Regret:  0.018785095056884138\n",
      "Percent Arms Correct:  0.5876470588235294\n",
      "51 Regret:  0.019057497843347777\n",
      "Percent Arms Correct:  0.5840384615384615\n",
      "52 Regret:  0.019213582648245513\n",
      "Percent Arms Correct:  0.5835849056603774\n",
      "53 Regret:  0.01914505425547422\n",
      "Percent Arms Correct:  0.5855555555555556\n",
      "54 Regret:  0.019361067660660907\n",
      "Percent Arms Correct:  0.5832727272727273\n",
      "55 Regret:  0.01916235500331303\n",
      "Percent Arms Correct:  0.5869642857142857\n",
      "56 Regret:  0.019069177078148515\n",
      "Percent Arms Correct:  0.5871929824561404\n",
      "57 Regret:  0.019654636299398183\n",
      "Percent Arms Correct:  0.5805172413793104\n",
      "58 Regret:  0.01942152797118208\n",
      "Percent Arms Correct:  0.5827118644067797\n",
      "59 Regret:  0.01919116680122291\n",
      "Percent Arms Correct:  0.5863333333333334\n",
      "60 Regret:  0.01918990074534763\n",
      "Percent Arms Correct:  0.5867213114754098\n",
      "61 Regret:  0.019103322670074\n",
      "Percent Arms Correct:  0.5875806451612903\n",
      "62 Regret:  0.019094985019090394\n",
      "Percent Arms Correct:  0.5876190476190477\n",
      "63 Regret:  0.01910269590371172\n",
      "Percent Arms Correct:  0.586875\n",
      "64 Regret:  0.01900736896965939\n",
      "Percent Arms Correct:  0.586923076923077\n",
      "65 Regret:  0.01884580082895065\n",
      "Percent Arms Correct:  0.5881818181818183\n",
      "66 Regret:  0.01880704526400277\n",
      "Percent Arms Correct:  0.5880597014925374\n",
      "67 Regret:  0.018852182153804117\n",
      "Percent Arms Correct:  0.5889705882352941\n",
      "68 Regret:  0.018592255288714787\n",
      "Percent Arms Correct:  0.5936231884057971\n",
      "69 Regret:  0.019227602882477057\n",
      "Percent Arms Correct:  0.5904285714285714\n",
      "70 Regret:  0.018988817471938705\n",
      "Percent Arms Correct:  0.5943661971830986\n",
      "71 Regret:  0.019012030434775323\n",
      "Percent Arms Correct:  0.5920833333333333\n",
      "72 Regret:  0.018818565855625607\n",
      "Percent Arms Correct:  0.594931506849315\n",
      "73 Regret:  0.018610237082120385\n",
      "Percent Arms Correct:  0.598108108108108\n",
      "74 Regret:  0.01863358543952927\n",
      "Percent Arms Correct:  0.5977333333333332\n",
      "75 Regret:  0.01842142012365481\n",
      "Percent Arms Correct:  0.5998684210526314\n",
      "76 Regret:  0.0182892589522949\n",
      "Percent Arms Correct:  0.6010389610389608\n",
      "77 Regret:  0.018163244222822145\n",
      "Percent Arms Correct:  0.6020512820512819\n",
      "78 Regret:  0.018119990466828634\n",
      "Percent Arms Correct:  0.6025316455696201\n",
      "79 Regret:  0.01802654996936326\n",
      "Percent Arms Correct:  0.6035\n",
      "80 Regret:  0.018254271806878854\n",
      "Percent Arms Correct:  0.601111111111111\n",
      "81 Regret:  0.01820617787633659\n",
      "Percent Arms Correct:  0.6007317073170731\n",
      "82 Regret:  0.018234757937249695\n",
      "Percent Arms Correct:  0.6001204819277108\n",
      "83 Regret:  0.018236070500625784\n",
      "Percent Arms Correct:  0.6002380952380951\n",
      "84 Regret:  0.018370904494761763\n",
      "Percent Arms Correct:  0.5991764705882352\n",
      "85 Regret:  0.018277811986356324\n",
      "Percent Arms Correct:  0.5994186046511627\n",
      "86 Regret:  0.01824667007761376\n",
      "Percent Arms Correct:  0.6018390804597701\n",
      "87 Regret:  0.01838971304625209\n",
      "Percent Arms Correct:  0.6003409090909091\n",
      "88 Regret:  0.0183536943441958\n",
      "Percent Arms Correct:  0.6010112359550561\n",
      "89 Regret:  0.018269006241138818\n",
      "Percent Arms Correct:  0.6015555555555555\n",
      "90 Regret:  0.018365696804971528\n",
      "Percent Arms Correct:  0.5998901098901098\n",
      "91 Regret:  0.01828739255142864\n",
      "Percent Arms Correct:  0.6001086956521738\n",
      "92 Regret:  0.01816534769773904\n",
      "Percent Arms Correct:  0.6017204301075268\n",
      "93 Regret:  0.01845063283071021\n",
      "Percent Arms Correct:  0.5988297872340425\n",
      "94 Regret:  0.01835449506638964\n",
      "Percent Arms Correct:  0.5976842105263157\n",
      "95 Regret:  0.018208327534011914\n",
      "Percent Arms Correct:  0.5991666666666667\n",
      "96 Regret:  0.01825578590031694\n",
      "Percent Arms Correct:  0.5978350515463918\n",
      "97 Regret:  0.018219082241483525\n",
      "Percent Arms Correct:  0.5987755102040816\n",
      "98 Regret:  0.018271923766617258\n",
      "Percent Arms Correct:  0.5970707070707071\n",
      "99 Regret:  0.018212867887341416\n",
      "Percent Arms Correct:  0.5968\n",
      "100 Regret:  0.018078877830919396\n",
      "Percent Arms Correct:  0.5983168316831683\n",
      "101 Regret:  0.018362001397797617\n",
      "Percent Arms Correct:  0.5957843137254902\n",
      "102 Regret:  0.0183260060255687\n",
      "Percent Arms Correct:  0.5966990291262136\n",
      "103 Regret:  0.01823045565945974\n",
      "Percent Arms Correct:  0.5974038461538461\n",
      "104 Regret:  0.018125546568938132\n",
      "Percent Arms Correct:  0.5993333333333333\n",
      "105 Regret:  0.018099233846922563\n",
      "Percent Arms Correct:  0.5999056603773584\n",
      "106 Regret:  0.018093347328031768\n",
      "Percent Arms Correct:  0.5998130841121495\n",
      "107 Regret:  0.017982041524158342\n",
      "Percent Arms Correct:  0.6008333333333332\n",
      "108 Regret:  0.017860117069885578\n",
      "Percent Arms Correct:  0.6023853211009172\n",
      "109 Regret:  0.017818283298666672\n",
      "Percent Arms Correct:  0.6022727272727272\n",
      "110 Regret:  0.01791067966018795\n",
      "Percent Arms Correct:  0.6017117117117117\n",
      "111 Regret:  0.018003878924251433\n",
      "Percent Arms Correct:  0.5995535714285714\n",
      "112 Regret:  0.01801455885520106\n",
      "Percent Arms Correct:  0.5998230088495574\n",
      "113 Regret:  0.018088092006796057\n",
      "Percent Arms Correct:  0.5998245614035086\n",
      "114 Regret:  0.01819050763176916\n",
      "Percent Arms Correct:  0.5981739130434781\n",
      "115 Regret:  0.018296200116185053\n",
      "Percent Arms Correct:  0.5975862068965515\n",
      "116 Regret:  0.01832547864297596\n",
      "Percent Arms Correct:  0.5965811965811965\n",
      "117 Regret:  0.018271369536474378\n",
      "Percent Arms Correct:  0.597372881355932\n",
      "118 Regret:  0.01824753111699878\n",
      "Percent Arms Correct:  0.5976470588235292\n",
      "119 Regret:  0.018138656924808552\n",
      "Percent Arms Correct:  0.5989166666666667\n",
      "120 Regret:  0.018110052132613432\n",
      "Percent Arms Correct:  0.5983471074380166\n",
      "121 Regret:  0.018027806857345077\n",
      "Percent Arms Correct:  0.5992622950819672\n",
      "122 Regret:  0.018064121606407642\n",
      "Percent Arms Correct:  0.5988617886178862\n",
      "123 Regret:  0.01804589301830293\n",
      "Percent Arms Correct:  0.5987903225806451\n",
      "124 Regret:  0.0182754983198829\n",
      "Percent Arms Correct:  0.59648\n",
      "125 Regret:  0.01832849968877059\n",
      "Percent Arms Correct:  0.5957142857142858\n",
      "126 Regret:  0.018261398236894263\n",
      "Percent Arms Correct:  0.5962204724409449\n",
      "127 Regret:  0.018143412455629004\n",
      "Percent Arms Correct:  0.59796875\n",
      "128 Regret:  0.018031056120706535\n",
      "Percent Arms Correct:  0.5997674418604652\n",
      "129 Regret:  0.01799604655825533\n",
      "Percent Arms Correct:  0.5989230769230769\n",
      "130 Regret:  0.017903920926441587\n",
      "Percent Arms Correct:  0.6002290076335878\n",
      "131 Regret:  0.017957662227369536\n",
      "Percent Arms Correct:  0.6003787878787878\n",
      "132 Regret:  0.01795141775155776\n",
      "Percent Arms Correct:  0.6005263157894737\n",
      "133 Regret:  0.017967473349356746\n",
      "Percent Arms Correct:  0.5991791044776119\n",
      "134 Regret:  0.017858130526642695\n",
      "Percent Arms Correct:  0.601037037037037\n",
      "135 Regret:  0.017836248892897002\n",
      "Percent Arms Correct:  0.6008088235294118\n",
      "136 Regret:  0.017903213974268577\n",
      "Percent Arms Correct:  0.5996350364963504\n",
      "137 Regret:  0.017911809206808633\n",
      "Percent Arms Correct:  0.5996376811594203\n",
      "138 Regret:  0.018077007937496326\n",
      "Percent Arms Correct:  0.5974820143884891\n",
      "139 Regret:  0.018081661400667925\n",
      "Percent Arms Correct:  0.5964999999999999\n",
      "140 Regret:  0.018120100874047567\n",
      "Percent Arms Correct:  0.5957446808510638\n",
      "141 Regret:  0.018229432306072057\n",
      "Percent Arms Correct:  0.5942957746478873\n",
      "142 Regret:  0.018193981148240685\n",
      "Percent Arms Correct:  0.5948251748251748\n",
      "143 Regret:  0.018260589762374164\n",
      "Percent Arms Correct:  0.5944444444444444\n",
      "144 Regret:  0.0181856577095961\n",
      "Percent Arms Correct:  0.5956551724137932\n",
      "145 Regret:  0.018106386203028593\n",
      "Percent Arms Correct:  0.5964383561643836\n",
      "146 Regret:  0.018118597627664934\n",
      "Percent Arms Correct:  0.5963265306122448\n",
      "147 Regret:  0.018164811036439386\n",
      "Percent Arms Correct:  0.5958783783783783\n",
      "148 Regret:  0.01845930837721395\n",
      "Percent Arms Correct:  0.5942953020134228\n",
      "149 Regret:  0.018425353079801424\n",
      "Percent Arms Correct:  0.5946\n",
      "150 Regret:  0.018354999965799985\n",
      "Percent Arms Correct:  0.5960264900662252\n",
      "151 Regret:  0.018345602178765387\n",
      "Percent Arms Correct:  0.5940789473684209\n",
      "152 Regret:  0.018305914510373222\n",
      "Percent Arms Correct:  0.5941176470588235\n",
      "153 Regret:  0.01833084424959145\n",
      "Percent Arms Correct:  0.592987012987013\n",
      "154 Regret:  0.01828930457610817\n",
      "Percent Arms Correct:  0.5932903225806451\n",
      "155 Regret:  0.018259267385777395\n",
      "Percent Arms Correct:  0.5925641025641025\n",
      "156 Regret:  0.018177095777280983\n",
      "Percent Arms Correct:  0.5940127388535031\n",
      "157 Regret:  0.018222386548310352\n",
      "Percent Arms Correct:  0.5935443037974684\n",
      "158 Regret:  0.018258933962319843\n",
      "Percent Arms Correct:  0.5929559748427673\n",
      "159 Regret:  0.018271097110482516\n",
      "Percent Arms Correct:  0.5924375000000001\n",
      "160 Regret:  0.018226652614763642\n",
      "Percent Arms Correct:  0.5928571428571429\n",
      "161 Regret:  0.01843908541285387\n",
      "Percent Arms Correct:  0.5910493827160493\n",
      "162 Regret:  0.01841265538548822\n",
      "Percent Arms Correct:  0.5912883435582822\n",
      "163 Regret:  0.018368345026917778\n",
      "Percent Arms Correct:  0.5914634146341463\n",
      "164 Regret:  0.018264145307793196\n",
      "Percent Arms Correct:  0.5933333333333334\n",
      "165 Regret:  0.01831877407167634\n",
      "Percent Arms Correct:  0.5934939759036144\n",
      "166 Regret:  0.018289336069935967\n",
      "Percent Arms Correct:  0.5938323353293412\n",
      "167 Regret:  0.018341069203632373\n",
      "Percent Arms Correct:  0.5933333333333334\n",
      "168 Regret:  0.018302764429107374\n",
      "Percent Arms Correct:  0.5937869822485208\n",
      "169 Regret:  0.018331814639147043\n",
      "Percent Arms Correct:  0.5930588235294119\n",
      "170 Regret:  0.01844920908006954\n",
      "Percent Arms Correct:  0.5912865497076024\n",
      "171 Regret:  0.01849679834565397\n",
      "Percent Arms Correct:  0.5905232558139535\n",
      "172 Regret:  0.018497007241879047\n",
      "Percent Arms Correct:  0.5903468208092486\n",
      "173 Regret:  0.01850586058538482\n",
      "Percent Arms Correct:  0.5910344827586207\n",
      "174 Regret:  0.01847260324517265\n",
      "Percent Arms Correct:  0.5917714285714286\n",
      "175 Regret:  0.01846794251171311\n",
      "Percent Arms Correct:  0.5917613636363636\n",
      "176 Regret:  0.018415418216408793\n",
      "Percent Arms Correct:  0.5928248587570621\n",
      "177 Regret:  0.018421976041935614\n",
      "Percent Arms Correct:  0.5920224719101124\n",
      "178 Regret:  0.018505391586434054\n",
      "Percent Arms Correct:  0.5912849162011173\n",
      "179 Regret:  0.018537756075966173\n",
      "Percent Arms Correct:  0.5908333333333333\n",
      "180 Regret:  0.018491315680815842\n",
      "Percent Arms Correct:  0.5911049723756906\n",
      "181 Regret:  0.018467721329416036\n",
      "Percent Arms Correct:  0.5914835164835165\n",
      "182 Regret:  0.01847676825397928\n",
      "Percent Arms Correct:  0.5914754098360655\n",
      "183 Regret:  0.018427923043023126\n",
      "Percent Arms Correct:  0.5922826086956521\n",
      "184 Regret:  0.01839196948756187\n",
      "Percent Arms Correct:  0.5926486486486485\n",
      "185 Regret:  0.01837796255193841\n",
      "Percent Arms Correct:  0.591774193548387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 66\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#move to next environment state \u001b[39;00m\n\u001b[1;32m     58\u001b[0m all_contexts, sampled_rewards, sampled_features, cur_step  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     59\u001b[0m     state_contexts \u001b[38;5;241m=\u001b[39m state_contexts, \n\u001b[1;32m     60\u001b[0m     action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     61\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m rewards \u001b[38;5;241m=\u001b[39m objective(\n\u001b[1;32m     65\u001b[0m     agent_actions \u001b[38;5;241m=\u001b[39m actions,\n\u001b[0;32m---> 66\u001b[0m     true_rewards \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_true_rewards(state_contexts, action_contexts)\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m cumul_regret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregret\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#update model state \u001b[39;00m\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/env/ranking_env.py:117\u001b[0m, in \u001b[0;36mRankingSyntheticEnv.get_true_rewards\u001b[0;34m(self, contexts, action_contexts)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_true_rewards\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    114\u001b[0m                     contexts: Tensor, \n\u001b[1;32m    115\u001b[0m                     action_contexts: Tensor):\n\u001b[0;32m--> 117\u001b[0m     features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_env\u001b[38;5;241m.\u001b[39mfeatures_all_arms(contexts, action_contexts), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    118\u001b[0m     true_rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkb,bd->nkd\u001b[39m\u001b[38;5;124m'\u001b[39m, features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_beta)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m true_rewards\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/model/pers_ranking_model.py:50\u001b[0m, in \u001b[0;36mPersonalizedRankingModel.features_all_arms\u001b[0;34m(self, contexts, action_contexts)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_arms):\n\u001b[1;32m     49\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i] \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[0;32m---> 50\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_map(actions, contexts, action_contexts)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/model/pers_ranking_model.py:39\u001b[0m, in \u001b[0;36mPersonalizedRankingModel.feature_map\u001b[0;34m(self, actions, contexts, action_contexts)\u001b[0m\n\u001b[1;32m     37\u001b[0m rankers \u001b[38;5;241m=\u001b[39m action_contexts[actions]\n\u001b[1;32m     38\u001b[0m contexts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf, nif -> nif\u001b[39m\u001b[38;5;124m'\u001b[39m, user_contexts, item_contexts)\n\u001b[0;32m---> 39\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnif, nf -> ni\u001b[39m\u001b[38;5;124m'\u001b[39m, contexts, rankers)\n\u001b[1;32m     40\u001b[0m max_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(scores, num_items, largest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m     41\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, num_items)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    cumul_regret = 0\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts = all_contexts \n",
    "        state_contexts = tuple(contexts.to(device) for contexts in state_contexts)\n",
    "        eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "        action_contexts = (action_contexts[0], action_contexts[1].to(device))\n",
    "        \n",
    "        #train agent \n",
    "        agent.train_agent( \n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            cur_step = cur_step, \n",
    "            n_steps = n_steps, \n",
    "            train_context_sampler = env.sample_train_contexts, \n",
    "            eval_contexts = eval_contexts,\n",
    "            eval_action_contexts = action_contexts, \n",
    "            real_batch = batch_size, \n",
    "            print_losses=False, \n",
    "            objective=objective,\n",
    "            repeats=10000\n",
    "        )   \n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective\n",
    "        ) \n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, torch.mean(probs, dim=0))\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        \n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = sampled_features, \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True)\n",
    "    eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "    #calculate results from objective\n",
    "    #fantasy_rewards = torch.randn(fantasy_rewards.shape) \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 1 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
