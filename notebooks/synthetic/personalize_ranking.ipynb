{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Formulation \n",
    "\n",
    "### Setting \n",
    "\n",
    "Each user has a context $x \\in \\mathbb{R}^d$ and each item to be recommended also has a context $a \\in \\mathbb{R}^d$. For each user $x$, there is a set of items $a_{1}, a_{2}, ..., a_{B}$ that can be recommended, and our goal is to choose a ranking policy such that we recommend a set of $b < B$ items optimally. \n",
    "\n",
    "To accomplish this, we want to choose a ranker $w \\in \\{w_{1}, w_{2}, ..., w_{K}\\}$ for each user $x$, where $w \\in \\mathbb{R}^d$. Note that we assume that there is a discrete set of rankers that we will choose, instead of continuously optimizing the ranking policy. Given a feature map $\\phi(x,a): \\mathbb{R}^d \\to \\mathbb{R}^d$, a given $w$ selects a set of $n$ items by taking $A_{b, x} := \\text{top} \\; b \\; \\{w^\\top \\phi(x,a)\\}$. \n",
    "\n",
    "### Model \n",
    "\n",
    "To choose $w$, we first assume that the true model is a linear model with parameter $\\theta \\in \\mathbb{R}^d$ such that $r(x,a; \\theta) = \\theta^\\top \\phi(x,a)$. In my initial model, I let $\\phi(x,a) := x \\odot a$ where $\\odot$ is the element-wise product. This leads to the interpretation that each parameter $\\theta$ or ranker $w_{i}$ is choosing the coefficients for a weighted inner product $\\langle x, a \\rangle _{w_{i}}$.\n",
    "\n",
    "Assume that we are at the last stage $T$ where we have our estimate $\\theta_{T}$. Then we would choose a ranker as follows: \n",
    "\n",
    "- Take a context $x$. For each ranker $w_{i}$, calculate the set of actions $A_{b,x,i} = \\text{top} \\; b \\; \\{\\langle x, a \\rangle _{w_{i}}\\}$ that would be taken if $w_{i}$ was chosen. \n",
    "\n",
    "- Given this set of actions $A_{b,x,i}$, calculate the fantasized rewards under $\\theta_{T}$ of choosing $A_{b,x,i}$: $r(x, w_{i}; \\theta_{T}) = \\sum_{a \\in A_{b,x,i}} \\langle x, a \\rangle _{\\theta_{T}}$  \n",
    "\n",
    "- Finally, for each $x$, we will choose $w_{x}$ by taking $w_{x} = \\argmax_{w_{i}} \\{r(x, w_{i})\\}$\n",
    "\n",
    "### Thompson Sampling \n",
    "\n",
    "We first calculate the sets $A_{b,x,i}$ that each ranker $w_{i}$ will choose. TS then samples a posterior sample $\\hat{\\theta}_{t} \\sim N(\\theta_{t}, \\Sigma_{t})$. Under $\\hat{\\theta}_{t}$, we calculate $r(x, w_{i}; \\hat{\\theta}_{t})$ and choose the arg max. \n",
    "\n",
    "### Initial Observations with Synthetic Data \n",
    "\n",
    "RHO works relatively well under this ranking model. \n",
    "\n",
    "TS does not outperform Uniform in basically all the settings I've tried. \n",
    "\n",
    "The gap between RHO and Uniform decreases as context dimension increases (which may be problematic as MIND embeddings are 100 dimensional, and BERT is 768 dimensional) \n",
    "\n",
    "I don't know if a logistic map is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import PersSyntheticEnv, RankingSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel, PersonalizedRankingModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho, RankingUniform, RankingTS, RankingRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 10\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.2 * torch.ones((n_days, 1))\n",
    "\n",
    "n_items = 4\n",
    "total_items = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*50)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len, scaling, n_objs=n_objs)\n",
    "user_context_mu, user_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "item_context_mu, item_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "\n",
    "\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedRankingModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = RankingSyntheticEnv(\n",
    "    true_env = model,\n",
    "    n_steps = n_steps,\n",
    "    user_context_mu = user_context_mu, \n",
    "    user_context_var = user_context_var,\n",
    "    item_context_mu = item_context_mu,\n",
    "    item_context_var = item_context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size,\n",
    "    n_arms = n_arms,\n",
    "    n_items = n_items,\n",
    "    total_items = total_items,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "torch.Size([100, 10]) torch.Size([100, 10, 10])\n",
      "4 tensor([[ 1.2242,  0.4046, -0.5610,  0.8038,  0.1894, -0.3527,  1.1927,  1.2778,\n",
      "          0.6996,  1.4482],\n",
      "        [ 1.4256,  0.6969,  0.2864,  1.2346, -1.3946, -0.5059,  0.7187,  0.6895,\n",
      "          1.4225,  0.5833],\n",
      "        [ 1.0783,  0.0752,  0.9702,  2.1014,  1.8436,  1.1151,  0.0830,  0.3832,\n",
      "          2.2211,  0.0084],\n",
      "        [ 1.9807,  0.8198,  1.6593,  1.7063,  0.2288,  0.7508,  1.0968, -0.0670,\n",
      "         -1.3840, -1.2977],\n",
      "        [ 1.0812,  0.8957,  1.7140,  2.1685,  2.2265, -0.5535,  0.7209,  0.0237,\n",
      "          2.0079, -0.2158],\n",
      "        [ 0.0059,  0.8364,  1.2669,  0.8025,  0.4772, -0.3791, -0.8101, -0.6798,\n",
      "          1.4521, -0.5376],\n",
      "        [ 1.8977,  1.4114,  0.4934,  0.5317,  0.0398,  0.9299,  1.2554, -1.0875,\n",
      "          2.5957,  1.4481],\n",
      "        [-0.0471,  1.0671, -0.3254,  2.4931,  1.2227,  3.4607,  2.7104,  0.7407,\n",
      "          3.1351,  1.6541],\n",
      "        [ 1.0538,  1.0085, -2.5757, -0.0690,  1.8072,  0.7711,  0.8188, -0.2725,\n",
      "          0.5978,  2.0686],\n",
      "        [ 1.8671,  0.3191,  0.8836,  0.8891,  0.6186,  0.6742,  0.3058,  2.2100,\n",
      "          0.3536,  0.6445]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print('hi')\n",
    "contexts, cur_step = env.reset()\n",
    "state_contexts, action_contexts, eval_contexts = contexts \n",
    "user_contexts, item_contexts = state_contexts\n",
    "n_items, ranking_contexts = action_contexts\n",
    "print(user_contexts.shape, item_contexts.shape)\n",
    "print(n_items, ranking_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent  \n",
    "agent = RankingUniform(model, \"Linear Uniform\")\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=False, n_samples = 100)\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "agent = RankingRho(model, \"Linear Rho\", lr=0.6, epochs = 10, weights = (1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.03211196400225162\n",
      "Percent Arms Correct:  0.47\n",
      "1 Regret:  0.031562709342688317\n",
      "Percent Arms Correct:  0.56\n",
      "2 Regret:  0.03206083172311385\n",
      "Percent Arms Correct:  0.57\n",
      "3 Regret:  0.03032291629351675\n",
      "Percent Arms Correct:  0.6025\n",
      "4 Regret:  0.031569915302097795\n",
      "Percent Arms Correct:  0.5820000000000001\n",
      "5 Regret:  0.02999806455336511\n",
      "Percent Arms Correct:  0.5816666666666667\n",
      "6 Regret:  0.03098558742286903\n",
      "Percent Arms Correct:  0.5828571428571429\n",
      "7 Regret:  0.032211670966353266\n",
      "Percent Arms Correct:  0.6075\n",
      "8 Regret:  0.03265379703500205\n",
      "Percent Arms Correct:  0.6033333333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, n_arms))\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m n_arms\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_context_sampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_train_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_contexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     48\u001b[0m         beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     49\u001b[0m         sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         objective \u001b[38;5;241m=\u001b[39m objective\n\u001b[1;32m     53\u001b[0m     ) \n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:83\u001b[0m, in \u001b[0;36mRankingRho.train_agent\u001b[0;34m(self, beta, sigma, cur_step, n_steps, train_context_sampler, eval_contexts, eval_action_contexts, real_batch, print_losses, objective, repeats)\u001b[0m\n\u001b[1;32m     81\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy(user_contexts)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#get fake covariance matrix\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mget_ranking_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_context_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_objs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)     \n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, train_features_all_arms, objective, probs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, msqrt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsqrt)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_losses \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy(user_contexts)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#get fake covariance matrix\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_ranking_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_context_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)     \n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, train_features_all_arms, objective, probs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, msqrt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsqrt)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_losses \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:102\u001b[0m, in \u001b[0;36mget_ranking_cov\u001b[0;34m(MDP, sigma, probs, features_list, action_contexts, cur_step, boost, obj, treat)\u001b[0m\n\u001b[1;32m     99\u001b[0m features_all_arms_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, features \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features_list):\n\u001b[0;32m--> 102\u001b[0m     features_all_arms \u001b[38;5;241m=\u001b[39m \u001b[43mMDP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_all_arms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     features_all_arms_list\u001b[38;5;241m.\u001b[39mappend(features_all_arms \u001b[38;5;241m/\u001b[39m (MDP\u001b[38;5;241m.\u001b[39ms2[cur_step \u001b[38;5;241m+\u001b[39m i][obj]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    104\u001b[0m features_all_arms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(features_all_arms_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(probs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/aexgym/aexgym/model/pers_ranking_model.py:52\u001b[0m, in \u001b[0;36mPersonalizedRankingModel.features_all_arms\u001b[0;34m(self, contexts, action_contexts)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_arms):\n\u001b[1;32m     51\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(ranker_contexts\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 52\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0, 1)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    torch.cuda.empty_cache()\n",
    "    cumul_regret = 0\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts = all_contexts \n",
    "        state_contexts = tuple(contexts.to(device) for contexts in state_contexts)\n",
    "        eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "        action_contexts = (action_contexts[0], action_contexts[1].to(device))\n",
    "        #train agent \n",
    "\n",
    "        if cur_step == 0:\n",
    "            probs = torch.ones((batch_size, n_arms)).to(device) / n_arms\n",
    "        else:\n",
    "            agent.train_agent( \n",
    "                beta = beta, \n",
    "                sigma = sigma, \n",
    "                cur_step = cur_step, \n",
    "                n_steps = n_steps, \n",
    "                train_context_sampler = env.sample_train_contexts, \n",
    "                eval_contexts = eval_contexts,\n",
    "                eval_action_contexts = action_contexts, \n",
    "                real_batch = batch_size, \n",
    "                print_losses=False, \n",
    "                objective=objective,\n",
    "                repeats=10000\n",
    "            )   \n",
    "\n",
    "            #get probabilities\n",
    "            probs = agent(\n",
    "                beta = beta, \n",
    "                sigma = sigma, \n",
    "                contexts = state_contexts, \n",
    "                action_contexts = action_contexts, \n",
    "                objective = objective\n",
    "            ) \n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        \n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = sampled_features, \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True)\n",
    "    eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "    #calculate results from objective\n",
    "    #fantasy_rewards = torch.randn(fantasy_rewards.shape) \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    results_dict['regret'] = 1 * cumul_regret / n_days + 0 * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 1 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
