{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Formulation \n",
    "\n",
    "### Setting \n",
    "\n",
    "Each user has a context $x \\in \\mathbb{R}^d$ and each item to be recommended also has a context $a \\in \\mathbb{R}^d$. For each user $x$, there is a set of items $a_{1}, a_{2}, ..., a_{B}$ that can be recommended, and our goal is to choose a ranking policy such that we recommend a set of $b < B$ items optimally. \n",
    "\n",
    "To accomplish this, we want to choose a ranker $w \\in \\{w_{1}, w_{2}, ..., w_{K}\\}$ for each user $x$, where $w \\in \\mathbb{R}^d$. Note that we assume that there is a discrete set of rankers that we will choose, instead of continuously optimizing the ranking policy. Given a feature map $\\phi(x,a): \\mathbb{R}^d \\to \\mathbb{R}^d$, a given $w$ selects a set of $n$ items by taking $A_{b, x} := \\text{top} \\; b \\; \\{w^\\top \\phi(x,a)\\}$. \n",
    "\n",
    "### Model \n",
    "\n",
    "To choose $w$, we first assume that the true model is a linear model with parameter $\\theta \\in \\mathbb{R}^d$ such that $r(x,a; \\theta) = \\theta^\\top \\phi(x,a)$. In my initial model, I let $\\phi(x,a) := x \\odot a$ where $\\odot$ is the element-wise product. This leads to the interpretation that each parameter $\\theta$ or ranker $w_{i}$ is choosing the coefficients for a weighted inner product $\\langle x, a \\rangle _{w_{i}}$.\n",
    "\n",
    "Assume that we are at the last stage $T$ where we have our estimate $\\theta_{T}$. Then we would choose a ranker as follows: \n",
    "\n",
    "- Take a context $x$. For each ranker $w_{i}$, calculate the set of actions $A_{b,x,i} = \\text{top} \\; b \\; \\{\\langle x, a \\rangle _{w_{i}}\\}$ that would be taken if $w_{i}$ was chosen. \n",
    "\n",
    "- Given this set of actions $A_{b,x,i}$, calculate the fantasized rewards under $\\theta_{T}$ of choosing $A_{b,x,i}$: $r(x, w_{i}; \\theta_{T}) = \\sum_{a \\in A_{b,x,i}} \\langle x, a \\rangle _{\\theta_{T}}$  \n",
    "\n",
    "- Finally, for each $x$, we will choose $w_{x}$ by taking $w_{x} = \\argmax_{w_{i}} \\{r(x, w_{i})\\}$\n",
    "\n",
    "### Thompson Sampling \n",
    "\n",
    "We first calculate the sets $A_{b,x,i}$ that each ranker $w_{i}$ will choose. TS then samples a posterior sample $\\hat{\\theta}_{t} \\sim N(\\theta_{t}, \\Sigma_{t})$. Under $\\hat{\\theta}_{t}$, we calculate $r(x, w_{i}; \\hat{\\theta}_{t})$ and choose the arg max. \n",
    "\n",
    "### Initial Observations with Synthetic Data \n",
    "\n",
    "RHO works relatively well under this ranking model. \n",
    "\n",
    "TS does not outperform Uniform in basically all the settings I've tried. \n",
    "\n",
    "The gap between RHO and Uniform decreases as context dimension increases (which may be problematic as MIND embeddings are 100 dimensional, and BERT is 768 dimensional) \n",
    "\n",
    "I don't know if a logistic map is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import PersSyntheticEnv, RankingSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel, PersonalizedRankingModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho, RankingUniform, RankingTS, RankingRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 10\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.1 * torch.ones((n_days, 1))\n",
    "\n",
    "n_items = 4\n",
    "total_items = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*50)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len, scaling, n_objs=n_objs)\n",
    "user_context_mu, user_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "item_context_mu, item_context_var = torch.ones(context_len), 0.5*torch.eye(context_len)\n",
    "\n",
    "\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedRankingModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = RankingSyntheticEnv(\n",
    "    true_env = model,\n",
    "    n_steps = n_steps,\n",
    "    user_context_mu = user_context_mu, \n",
    "    user_context_var = user_context_var,\n",
    "    item_context_mu = item_context_mu,\n",
    "    item_context_var = item_context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size,\n",
    "    n_arms = n_arms,\n",
    "    n_items = n_items,\n",
    "    total_items = total_items,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "torch.Size([100, 10]) torch.Size([100, 10, 10])\n",
      "4 tensor([[ 0.7489,  1.9198, -1.6020,  0.7566, -0.0931, -0.0515,  2.0877,  0.6241,\n",
      "          0.9282,  0.6370],\n",
      "        [ 0.2164,  0.6272,  0.7729,  1.8130,  1.8139,  1.7961,  1.2143,  0.8157,\n",
      "          2.5399,  2.9441],\n",
      "        [-0.0117,  1.3500,  2.1109,  0.8507,  1.6477,  1.3418,  2.0567,  1.5093,\n",
      "          2.2464,  0.5344],\n",
      "        [ 0.9377,  1.9172,  1.1213,  0.9987,  1.3706,  0.2708, -0.1885,  2.4495,\n",
      "          1.1683,  0.5756],\n",
      "        [ 1.2740, -0.4079,  1.1152,  0.0798,  0.7816, -0.9583,  0.9531,  0.2683,\n",
      "          1.4087,  2.5125],\n",
      "        [-0.6295,  1.7201,  0.9255,  1.0354,  0.4899,  1.3011, -0.0904,  1.0129,\n",
      "         -0.5177,  0.3822],\n",
      "        [ 2.7619,  2.0693,  2.0282,  1.9656,  0.0388, -0.5507,  2.5061,  1.4310,\n",
      "          0.0887,  0.0161],\n",
      "        [ 2.0091,  1.3069,  0.5172,  1.0231, -0.3847,  1.7673,  0.1317,  2.0058,\n",
      "         -0.0287,  0.6663],\n",
      "        [ 1.4669,  0.9859, -0.5597,  1.9165, -0.6792, -0.7416,  1.8100,  1.8944,\n",
      "          0.9835,  1.2785],\n",
      "        [ 2.1080,  0.8965,  1.2032,  1.3260,  1.9281,  0.4847,  0.5745,  1.8405,\n",
      "         -0.1642,  1.6265]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print('hi')\n",
    "contexts, cur_step = env.reset()\n",
    "state_contexts, action_contexts, eval_contexts = contexts \n",
    "user_contexts, item_contexts = state_contexts\n",
    "n_items, ranking_contexts = action_contexts\n",
    "print(user_contexts.shape, item_contexts.shape)\n",
    "print(n_items, ranking_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent  \n",
    "agent = RankingUniform(model, \"Linear Uniform\")\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=False, n_samples = 100)\n",
    "#agent = RankingTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "agent = RankingRho(model, \"Linear Rho\", lr=0.4, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.031683875061571595\n",
      "Percent Arms Correct:  0.58\n",
      "1 Regret:  0.03463939996436238\n",
      "Percent Arms Correct:  0.63\n",
      "2 Regret:  0.0338411392023166\n",
      "Percent Arms Correct:  0.6533333333333333\n",
      "3 Regret:  0.03175222557038069\n",
      "Percent Arms Correct:  0.6699999999999999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, n_arms))\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m n_arms\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_context_sampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_train_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_contexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     48\u001b[0m         beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     49\u001b[0m         sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         objective \u001b[38;5;241m=\u001b[39m objective\n\u001b[1;32m     53\u001b[0m     ) \n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:81\u001b[0m, in \u001b[0;36mRankingRho.train_agent\u001b[0;34m(self, beta, sigma, cur_step, n_steps, train_context_sampler, eval_contexts, eval_action_contexts, real_batch, print_losses, objective, repeats)\u001b[0m\n\u001b[1;32m     79\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy(user_contexts)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#get fake covariance matrix\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mget_ranking_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_context_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_objs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)     \n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, train_features_all_arms, objective, probs, msqrt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsqrt)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_losses \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:81\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy(user_contexts)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#get fake covariance matrix\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_ranking_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_context_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_action_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)     \n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, train_features_all_arms, objective, probs, msqrt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsqrt)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_losses \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/aexgym/aexgym/agent/ranking/ranking_rho.py:100\u001b[0m, in \u001b[0;36mget_ranking_cov\u001b[0;34m(MDP, sigma, probs, features_list, action_contexts, cur_step, boost, obj, treat)\u001b[0m\n\u001b[1;32m     97\u001b[0m features_all_arms_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, features \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features_list):\n\u001b[0;32m--> 100\u001b[0m     features_all_arms \u001b[38;5;241m=\u001b[39m \u001b[43mMDP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_all_arms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     features_all_arms_list\u001b[38;5;241m.\u001b[39mappend(features_all_arms \u001b[38;5;241m/\u001b[39m (MDP\u001b[38;5;241m.\u001b[39ms2[cur_step \u001b[38;5;241m+\u001b[39m i][obj]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    102\u001b[0m features_all_arms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(features_all_arms_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(probs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/aexgym/aexgym/model/pers_ranking_model.py:52\u001b[0m, in \u001b[0;36mPersonalizedRankingModel.features_all_arms\u001b[0;34m(self, contexts, action_contexts)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_arms):\n\u001b[1;32m     51\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(ranker_contexts\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 52\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_contexts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/aexgym/aexgym/model/pers_ranking_model.py:38\u001b[0m, in \u001b[0;36mPersonalizedRankingModel.feature_map\u001b[0;34m(self, actions, contexts, action_contexts)\u001b[0m\n\u001b[1;32m     36\u001b[0m context_len \u001b[38;5;241m=\u001b[39m user_contexts\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m rankers \u001b[38;5;241m=\u001b[39m action_contexts[actions]\n\u001b[0;32m---> 38\u001b[0m contexts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnf, nif -> nif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnif, nf -> ni\u001b[39m\u001b[38;5;124m'\u001b[39m, contexts, rankers)\n\u001b[1;32m     40\u001b[0m max_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(scores, num_items, largest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mindices\n",
      "File \u001b[0;32m~/.conda/envs/o3d/lib/python3.11/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0.3, 0.7)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    torch.cuda.empty_cache()\n",
    "    cumul_regret = 0\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts = all_contexts \n",
    "        state_contexts = tuple(contexts.to(device) for contexts in state_contexts)\n",
    "        eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "        action_contexts = (action_contexts[0], action_contexts[1].to(device))\n",
    "        #train agent \n",
    "\n",
    "        if cur_step == 0:\n",
    "            probs = torch.ones((batch_size, n_arms)).to(device) / n_arms\n",
    "        else:\n",
    "            agent.train_agent( \n",
    "                beta = beta, \n",
    "                sigma = sigma, \n",
    "                cur_step = cur_step, \n",
    "                n_steps = n_steps, \n",
    "                train_context_sampler = env.sample_train_contexts, \n",
    "                eval_contexts = eval_contexts,\n",
    "                eval_action_contexts = action_contexts, \n",
    "                real_batch = batch_size, \n",
    "                print_losses=False, \n",
    "                objective=objective,\n",
    "                repeats=10000\n",
    "            )   \n",
    "\n",
    "            #get probabilities\n",
    "            probs = agent(\n",
    "                beta = beta, \n",
    "                sigma = sigma, \n",
    "                contexts = state_contexts, \n",
    "                action_contexts = action_contexts, \n",
    "                objective = objective\n",
    "            ) \n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        \n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = sampled_features, \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True)\n",
    "    eval_contexts = tuple(contexts.to(device) for contexts in eval_contexts)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "    #calculate results from objective\n",
    "    #fantasy_rewards = torch.randn(fantasy_rewards.shape) \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    results_dict['regret'] = 1 * cumul_regret / n_days + 0 * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 1 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
